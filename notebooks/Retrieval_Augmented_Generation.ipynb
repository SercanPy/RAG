{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last modified: 07/11/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "This notebook uses the [IBM Granite 3.0 2B Instruct](https://huggingface.co/ibm-granite/granite-3.0-2b-instruct) model from HuggingFace. The model is hosted locally, and used within a LangChain RAG pipeline to query local data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components making up a RAG application are as follows\n",
    "- Data ingestion & pre-processing\n",
    "- Data embedding\n",
    "- Data storage (Vector db)\n",
    "- Prompt template\n",
    "- Retriever\n",
    "- Augmented generative model\n",
    "\n",
    "[Optional]\n",
    "- User interface\n",
    "- Backend API\n",
    "\n",
    "An example workflow is:\n",
    "1. Ingestion: Use Tika/Textract to extract text from private files.\n",
    "2. Preprocessing: Use NLTK/spaCy to clean and tokenize.\n",
    "3. Embedding: Generate embeddings with Hugging Face SentenceTransformers.\n",
    "4. Storage: Store embeddings in FAISS or Chroma.\n",
    "4. Retrieval: Use Haystack or LangChain to retrieve relevant document embeddings for a given query.\n",
    "5. Generation: Use Hugging Face Transformers or OpenAI’s API (optional) to generate responses based on retrieved content.\n",
    "6. Serve: Create an API with FastAPI and optionally a UI with Streamlit or Gradio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG\n",
    "Following the [LangChain demo](https://python.langchain.com/docs/tutorials/rag/), this sections sets up a RAG pipeline for single-task inferencing to query some dummy data stored using an in-memory vector database (FAISS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HuggingFaceHub` is used to access HuggingFace hosted models (via API).\n",
    "`HuggingFacePipeline` is used to access locally hosted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --upgrade langchain langchain-community python-dotenv\n",
    "# %pip install langchain-chroma # for Chroma db\n",
    "# %pip install faiss-cpu # for FAISS db\n",
    "\n",
    "# # for remotely hosted inference\n",
    "# pip install huggingface_hub\n",
    "\n",
    "# # for local inference\n",
    "# pip install langchain-huggingface transformers[torch] sentence-transformers\n",
    "\n",
    "# # for progress bar in Jupyter notebooks\n",
    "# %pip install --quiet tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up your API tokens\n",
    "load_dotenv(os.getcwd() + \"/.env\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\", \"\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding\n",
    "This is needed for a RAG application. Need to be able to embded documents to be stored in a vector database and later retrieved.\n",
    "\n",
    "Can use \n",
    "- **Hugging Face Transformers**: provides a wide range of pre-trained models (e.g. BERT, Sentence-BERT) for generating embeddings from text, images, and other data types. It’s highly versatile and integrates well with FAISS for tasks like semantic search.\n",
    "- **Sentence Transformers**: Built on top of Hugging Face Transformers, this library is specifically designed for creating high-quality sentence and document embeddings. It supports various models optimized for different tasks, making it a great choice for embedding text data.\n",
    "- **OpenAI Embeddings**: OpenAI offers models like GPT-3 and GPT-4 that can generate embeddings for text. These embeddings can be indexed and searched using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for embeddings using LangChain's HuggingFaceEmbeddings class\n",
    "from langchain_huggingface.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding_model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=embedding_model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    multi_process=False,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can explore embedding output with a test string\n",
    "\n",
    "# query_embedding = embeddings.embed_query(\"a test string\")\n",
    "# query_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "\n",
    "Store embedded data for retrieval by the RAG pipelines.\n",
    "\n",
    "Use one of the following for local development and testing\n",
    "- FAISS\n",
    "- Chroma db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, use in-memory vector store which will be lost once the kernel is stopped. Data can either be loaded from the file system, or input using the `Document` class in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy documents using LangChain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Malikai is a Data Science Consultant at Terox with over 3 years of professional experience in the transportation domain.\",\n",
    "        metadata={\"source\": \"Malikai-bio\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Robert Isling has earned his PhD certificate in Physics from the University College London in 2021.\",\n",
    "        metadata={\"source\": \"Robert-bio\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Nusret is originally from the city of Izmir in Turkey, but currently resides in Nottingham, United Kindom.\",\n",
    "        metadata={\"source\": \"Nusret-bio\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading dummy data from disk\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# loader = TextLoader(\"./test_rag_doc.txt\")\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Generate embeddings and store in FAISS\n",
    "faiss_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST that documents have been loaded into the vector store\n",
    "faiss_store.similarity_search_with_score(\"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace model path\n",
    "model_path = \"ibm-granite/granite-3.0-2b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remote model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using a model hosted remotely on HuggingFace infrastructure\n",
    "# from langchain import HuggingFaceHub\n",
    "\n",
    "# # Initialize Hugging Face model through LangChain's HuggingFaceHub\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=model_path,  # or any model of choice\n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_length\": 100},\n",
    "#     huggingfacehub_api_token=HUGGINGFACE_API_KEY,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a locally hosted model\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load a local model pipeline\n",
    "local_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=500,  # default is 20 tokens\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "# Initialize Hugging Face model through LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=local_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# The `PromptTemplate` class is suitable for one-off tasks, i.e.\n",
    "# not used for conversational interactions. It does not support\n",
    "# `{roles}` within the message.\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question by rephrasing the information provided in the context.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # The `ChatPromptTemplate` is suitable for conversational interactions\n",
    "# # with the LLM. It supports `{roles}` within the message, such as\n",
    "# # system prompt, ai prompt, and human prompt using the appropriate\n",
    "# # prompt template classes for each role.\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector store can be used as a retriever [source](https://python.langchain.com/docs/tutorials/retrievers/#retrievers). Vector stores can be queried by [source](https://python.langchain.com/docs/tutorials/retrievers/#vector-stores) \n",
    "- similarity\n",
    "- maximum marginal relevance (to balance similarity with query to diversity in retrieved results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Query by `similarity`\n",
    "# Return top match (k=1)\n",
    "retriever = RunnableLambda(faiss_store.similarity_search).bind(k=1)\n",
    "\n",
    "# # Alternative implementation\n",
    "# retriever = faiss_store..as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     search_kwargs={\"k\": 1},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the retriever\n",
    "# retriever.invoke(\"university\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RAG chain in LangChain syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Nusret?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Call\n",
    "Simply calling the IBM Granite 3.0 2b instruct model with a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_path = \"ibm-granite/granite-3.0-2b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input text as desired\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the dense transformer architecture?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "chat = tokenizer.apply_chat_template(\n",
    "    chat, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# tokenize the text\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# print output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].split(\"end_of_role|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
